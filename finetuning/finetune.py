# -*- coding: utf-8 -*-
"""finetune.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/112Y-K2o4I9iGYOF-Wvbs2Iu6PYvsGiPf

This assumes your have uploaded the finetuning subdirectory from the project repo [here](https://github.com/forrestdavis/ImplicitCausality). Change the path variable below accordingly. The results of the finetuning are saved under models.
"""

from google.colab import drive
drive.mount('/content/drive')

# Install `transformers` from master
!pip install transformers
!pip install datasets
#!pip install sentencepiece
!pip list | grep -E 'transformers|tokenizers'
# transformers version at notebook update --- 2.11.0
# tokenizers version at notebook update --- 0.8.0rc1

"""# **Import Necessary Packages**"""

from transformers import BertForPreTraining, Trainer, TrainingArguments
from transformers import LineByLineTextDataset
from transformers import AutoTokenizer, AutoModel
from transformers import AutoModelForMaskedLM
from datasets import load_dataset
from transformers import DataCollatorForLanguageModeling

"""# **Define Functions**"""

block_size = 128
def group_texts(examples):
    # Concatenate all texts.
    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
    total_length = len(concatenated_examples[list(examples.keys())[0]])
    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can
        # customize this part to your needs.
    total_length = (total_length // block_size) * block_size
    # Split by chunks of max_len.
    result = {
        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]
        for k, t in concatenated_examples.items()
    }
    result["labels"] = result["input_ids"].copy()
    return result

def create_dataset(tokenizer, train_file, valid_file):
    data = load_dataset("text", data_files={"train": train_file, "validation": valid_file})
    
    def tokenize_function(examples):
        return tokenizer(examples["text"])
    
    tokenized_data = data.map(tokenize_function, batched=True, num_proc=4, remove_columns=["text"])
    lm_datasets = tokenized_data.map(
    group_texts,
    batched=True,
    batch_size=1000,
    num_proc=4,
    )
    return lm_datasets

def get_trainer(model_file, train_file, valid_file, model_output_dir):

    model = AutoModelForMaskedLM.from_pretrained(model_file)
    tokenizer = AutoTokenizer.from_pretrained(model_file, do_lower_case=True, use_fast=False)
    lm_datasets = create_dataset(tokenizer, train_file, valid_file)

    print(tokenizer.decode(lm_datasets["train"][1]["input_ids"]))

    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)

    training_args = TrainingArguments(
        output_dir=model_output_dir,          # output directory
        overwrite_output_dir=True,
        num_train_epochs=3,              # total # of training epochs
        per_device_train_batch_size=16,  # batch size per device during training
        per_device_eval_batch_size=64,   # batch size for evaluation
        warmup_steps=500,                # number of warmup steps for learning rate scheduler
        weight_decay=0.01,               # strength of weight decay
        logging_dir=path+'logs',            # directory for storing logs
          )

    trainer = Trainer(
        model=model,                         # the instantiated ðŸ¤— Transformers model to be trained
        args=training_args,                  # training arguments, defined above
        train_dataset=lm_datasets["train"],
        eval_dataset=lm_datasets["validation"],        # evaluation dataset
        data_collator=data_collator,
      )
    
    return trainer

### Change to the location of your google drive location
path = '/content/drive/MyDrive/Dissertation/Implicit Causality/finetuning/'

"""# **Train Baseline Models**"""

model_files = ["dccuchile/bert-base-spanish-wwm-uncased", "mrm8488/RuPERTa-base", # Spanish
               "idb-ita/gilberto-uncased-from-camembert", 'Musixmatch/umberto-wikipedia-uncased-v1', "dbmdz/bert-base-italian-uncased", #Italian
               "bert-base-uncased", "roberta-base", # English
               "hfl/chinese-bert-wwm-ext", "hfl/chinese-roberta-wwm-ext" #Chinese
               ]

train_files = [path+'data/es_train_finetuning_base', 
               path+'data/es_train_finetuning_base', 
               path+'data/it_train_finetuning_base', 
               path+'data/it_train_finetuning_base',
               path+'data/it_train_finetuning_base', 
               path+'data/en_train_finetuning_base',
               path+'data/en_train_finetuning_base',
               path+'data/zh_train_finetuning_base',
               path+'data/zh_train_finetuning_base']

valid_files = [path+'data/es_valid_finetuning_base',
               path+'data/es_valid_finetuning_base',
               path+'data/it_valid_finetuning_base',
               path+'data/it_valid_finetuning_base',
               path+'data/it_valid_finetuning_base', 
               path+'data/en_valid_finetuning_base',
               path+'data/en_valid_finetuning_base',
               path+'data/zh_valid_finetuning_base',
               path+'data/zh_valid_finetuning_base']

model_output_dirs = [path+'models/ProBETO_BASE', 
                     path+'models/ProRuPERTa_BASE',
                     path+'models/ProGilBERTo_BASE',
                     path+'models/ProUmBERTo_BASE', 
                     path+'models/ProITALIAN_BASE', 
                     path+'models/ProBERT_BASE', 
                     path+'models/ProRoBERTa_BASE',
                     path+'models/ProCHINESE_BASE',
                     path+'models/ProROCHINESE_BASE']


for x, model_file in enumerate(model_files):

    train_file = train_files[x]
    valid_file = valid_files[x]
    model_output_dir = model_output_dirs[x]
    print(model_output_dir)

    trainer = get_trainer(model_file, train_file, valid_file, model_output_dir)
    trainer.train()
    trainer.save_model(model_output_dir)
    # save tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_file, use_fast=False)
    tokenizer.save_pretrained(model_output_dir)

"""# **Train Adapt Models**"""

model_files = ["dccuchile/bert-base-spanish-wwm-uncased", "mrm8488/RuPERTa-base", # Spanish
               "idb-ita/gilberto-uncased-from-camembert", 'Musixmatch/umberto-wikipedia-uncased-v1', "dbmdz/bert-base-italian-uncased", #Italian
               "bert-base-uncased", "roberta-base", # English
               "hfl/chinese-bert-wwm-ext", "hfl/chinese-roberta-wwm-ext" #Chinese
               ]

train_files = [path+'data/es_train_finetuning', 
               path+'data/es_train_finetuning', 
               path+'data/it_train_finetuning', 
               path+'data/it_train_finetuning',
               path+'data/it_train_finetuning', 
               path+'data/en_train_finetuning',
               path+'data/en_train_finetuning',
               path+'data/zh_train_finetuning',
               path+'data/zh_train_finetuning']

valid_files = [path+'data/es_valid_finetuning',
               path+'data/es_valid_finetuning',
               path+'data/it_valid_finetuning',
               path+'data/it_valid_finetuning',
               path+'data/it_valid_finetuning', 
               path+'data/en_valid_finetuning',
               path+'data/en_valid_finetuning',
               path+'data/zh_valid_finetuning',
               path+'data/zh_valid_finetuning']

model_output_dirs = [path+'models/ProBETO', 
                     '/content/drive/MyDrive/Dissertation/Implicit Causality/Finetune/ProRuPERTa',
                     '/content/drive/MyDrive/Dissertation/Implicit Causality/Finetune/ProGilBERTo',
                     '/content/drive/MyDrive/Dissertation/Implicit Causality/Finetune/ProUmBERTo', 
                     '/content/drive/MyDrive/Dissertation/Implicit Causality/Finetune/ProITALIAN', 
                     '/content/drive/MyDrive/Dissertation/Implicit Causality/Finetune/ProBERT', 
                     '/content/drive/MyDrive/Dissertation/Implicit Causality/Finetune/ProRoBERTa',
                     '/content/drive/MyDrive/Dissertation/Implicit Causality/Finetune/ProCHINESE',
                     '/content/drive/MyDrive/Dissertation/Implicit Causality/Finetune/ProROCHINESE']

for x, model_file in enumerate(model_files):
   
    train_file = train_files[x]
    valid_file = valid_files[x]
    model_output_dir = model_output_dirs[x]
    print(model_output_dir)

    trainer = get_trainer(model_file, train_file, valid_file, model_output_dir)
    trainer.train()
    trainer.save_model(model_output_dir)
    # save tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_file, use_fast=False)
    tokenizer.save_pretrained(model_output_dir)